{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# use for a \"no activation\" layer\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_deriv(post_act):\n",
    "    return np.ones_like(post_act)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(post_act):\n",
    "    return 1-post_act**2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f57ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(in_size: int, out_size: int, params: dict, name: str='' ):\n",
    "\n",
    "    W, b = None, None\n",
    "    a=in_size+out_size\n",
    "    c=2/a\n",
    "    #W = np.random.uniform(-c,c, size=(in_size, out_size))\n",
    "    W=np.random.uniform(-np.sqrt(6/(in_size+out_size)),np.sqrt(6/(in_size+out_size)),(in_size,out_size))\n",
    "    #W=np.random.uniform(-c,c)\n",
    "    #b=np.random.uniform(-c,c,size=(out_size))\n",
    "    b=np.zeros(out_size)\n",
    "    ## compute W and b using in_size and out_size.\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "    params['W' + name] = W\n",
    "    params['b' + name] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a7593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X: np.ndarray):\n",
    "\n",
    "    a=np.exp(-X)\n",
    "    a=a+1\n",
    "    #res=1/a\n",
    "    res=1/(1+np.exp(-X))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X: np.ndarray, params: dict, name: str='',\n",
    "            activation: callable=sigmoid):\n",
    "    pre_act, post_act = None, None\n",
    "    # get the layer parameters\n",
    "    W = params['W' + name]\n",
    "    b = params['b' + name]\n",
    "    #print(params)\n",
    "    #pre_act=X@W+b\n",
    "    pre_act = np.dot(X,W)+b\n",
    "    post_act=activation(pre_act)\n",
    "    ## compute pre_act using X, W and b.\n",
    "    ## compute post_act using pre_act.\n",
    "    # YOUR CODE HERE\n",
    "#    raise NotImplementedError()\n",
    "\n",
    "    # store the pre-activation and post-activation values\n",
    "    # these will be important in backprop\n",
    "    params['cache_' + name] = (X, pre_act, post_act)\n",
    "\n",
    "    return post_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X: np.ndarray):\n",
    "\n",
    "    a = np.max(X,axis=1)\n",
    "    b= X - np.expand_dims(a, axis=1)\n",
    "    a=np.exp(b)\n",
    "    \n",
    "    res=a/np.expand_dims(np.sum(a, axis=1),axis=1)\n",
    "#    res = None\n",
    "    \n",
    "    ## compute res using X\n",
    "    # YOUR CODE HERE\n",
    " #   raise NotImplementedError()\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_and_acc(y: np.ndarray, probs: np.ndarray):\n",
    "\n",
    "    loss, acc = None, None\n",
    "    a=np.log(probs)\n",
    "    a=y*a\n",
    "    a=np.sum(a)\n",
    "    loss=-a\n",
    "    \n",
    "    #n=np.zeros(0)\n",
    "    #for i in range(probs.shape[0]):\n",
    "     #   a=probs[i,:].max()\n",
    "      #  n=np.append(n,a)\n",
    "    a=y.astype(int)\n",
    "    pred=(probs == np.expand_dims(np.max(probs, axis=1), axis=1))\n",
    "    b=np.where(np.sum(pred, axis=1) > 1)[0]\n",
    "    #print(b)\n",
    "    c=b.shape[0]\n",
    "    for i in range(c):\n",
    "        pred[i, np.where(pred[i, :]==np.max(pred[i, :]))[0][0]+1:] = False\n",
    "    c= np.sum(np.abs(pred-a))//2\n",
    "    d=(a.shape[0]-c)/a.shape[0]\n",
    "    acc=d\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fdc97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(post_act: np.ndarray):\n",
    "\n",
    "    res = post_act*(1.0-post_act)\n",
    "    return res\n",
    "\n",
    "\n",
    "def backwards(delta: np.ndarray, params: dict, name: str='',\n",
    "              activation_deriv: callable=sigmoid_deriv):\n",
    "\n",
    "    grad_X, grad_W, grad_b = None, None, None\n",
    "    # everything you may need for this layer\n",
    "    W = params['W' + name]\n",
    "    b = params['b' + name]\n",
    "    X, pre_act, post_act = params['cache_' + name]\n",
    "    \n",
    "    grad_W= np.zeros(W.shape)\n",
    "    grad_X=np.zeros(X.shape)\n",
    "    grad_b=np.zeros(b.shape)\n",
    "    a=delta*activation_deriv(post_act)\n",
    "    #print(delta.shape)\n",
    "    #print(a.shape)\n",
    "    #print(X.shape)\n",
    "    #print(W.shape)\n",
    "    #print(b.shape)\n",
    "    #grad_W=delta*a*X\n",
    "    #grad_X=a*delta*W\n",
    "    #grad_b=a*delta\n",
    "    for i in range(X.shape[0]):\n",
    "        grad_W= grad_W+np.dot(np.expand_dims(X[i, :], axis=1), np.expand_dims(a[i, :], axis=0))\n",
    "        grad_b=grad_b+a[i,:]\n",
    "        grad_X[i,:]=np.dot(W, np.expand_dims(a[i, :],axis=1)).reshape([-1])\n",
    "    \n",
    "    \n",
    "    #print(grad_X.shape)\n",
    "    # do the derivative through activation first\n",
    "    # then compute the derivative W,b, and X\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "    # store the gradients\n",
    "    params['grad_W' + name] = grad_W\n",
    "    params['grad_b' + name] = grad_b\n",
    "    return grad_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(X: np.ndarray, params: dict, name: str='',\n",
    "            stride: int=1, pad: int=0):\n",
    "\n",
    "    res = None\n",
    "    # get the layer parameters\n",
    "    w = params['W' + name] # Conv Filter weights [F x C x HH x WW]\n",
    "    b = params['b' + name] # Biases [F]\n",
    "\n",
    "    \n",
    "    Xp = np.pad(X, ((0,0), (0,0), (pad,pad), (pad,pad)))\n",
    "    #X=np.pad(X,(row_padding, col_padding), 'constant', constant_values=(pad, pad))\n",
    "    n=X.shape[0]\n",
    "    ch=X.shape[1]\n",
    "    h=X.shape[2]\n",
    "    wi=X.shape[3]\n",
    "    #print(a,n,c,d)\n",
    "    f=w.shape[0]\n",
    "    fch=w.shape[1]\n",
    "    fh=w.shape[2]\n",
    "    fw=w.shape[3] \n",
    "    \n",
    "    hf = int((h + 2*pad - fh)/stride) + 1\n",
    "    wf =int((wi+ 2*pad - fw)/stride ) + 1\n",
    "    #print(e,f,g,h)\n",
    "    windows=[]\n",
    "    wsize=ch*fw*fh\n",
    "    #new= np.zeros([n,ch, hf, wf])\n",
    "    #print(new.shape)\n",
    "    \n",
    "    for i in range(0, Xp.shape[2], stride):\n",
    "        for j in range(0,Xp.shape[3], stride):\n",
    "            window = Xp[:, :, i: i + fh, j: j + fw].reshape(n, -1)\n",
    "            \n",
    "            if window.shape[1] == wsize:\n",
    "                windows.append(window.reshape(-1))\n",
    "    windows = np.concatenate(windows).reshape(len(windows), n, -1)\n",
    "    out = []\n",
    "    q = w.reshape(f, -1).T\n",
    "    for i in range(windows.shape[0]):  \n",
    "        out.append((windows[i] @q+b).reshape(n,f))\n",
    "        \n",
    "        \n",
    "    hf = int((h + 2*pad - fh)//stride) + 1\n",
    "    wf =int((wi+ 2*pad - fw)//stride ) + 1\n",
    "    new= np.stack(out, axis=-1).reshape(n,f,hf,wf) \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    #res=Xp@w+b\n",
    "    # store the input and convolution parameters\n",
    "    # these will be important in backprop\n",
    "    params['cache_' + name] = (X, stride, pad)\n",
    "    res=new\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(delta: np.ndarray, params: dict, name: str=''):\n",
    "\n",
    "    grad_X, grad_W, grad_b = None, None, None\n",
    "    # everything you may need for this layer\n",
    "    W = params['W' + name]\n",
    "    b = params['b' + name]\n",
    "    X, stride, pad = params['cache_' + name]\n",
    "\n",
    "    # compute the derivative W,b, and X\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    n=X.shape[0]\n",
    "    grad_X=np.zeros(X.shape)\n",
    "    grad_W=np.zeros(W.shape)\n",
    "    H=X.shape[2]\n",
    "    w=X.shape[3]\n",
    "    f=W.shape[0] \n",
    "    C=W.shape[1]\n",
    "    HH=W.shape[2]\n",
    "    WW=W.shape[3]\n",
    "    H1 = int(1 + ((H -HH+2*pad)/stride))\n",
    "    W1 = int(1 + ((w - WW+2*pad)/stride))  \n",
    "    dout = delta\n",
    "    Xp = np.pad(X, ((0,0), (0,0), (pad,pad), (pad,pad)))\n",
    "    for n1 in range(n):\n",
    "        for h in range(H):\n",
    "            for m in range(w):\n",
    "                y_idx=[]\n",
    "                w_idx=[]\n",
    "                \n",
    "                for i in range(H1):\n",
    "                    for j in range(W1):\n",
    "                        if ((h + pad - i * stride) >= 0) and ((h + pad - i * stride) < HH) and ((m + pad - j * stride) >= 0) and ((m + pad - j * stride) < WW):\n",
    "                            w_idx.append((h + pad - i * stride, m + pad - j * stride))\n",
    "                            y_idx.append((i, j))\n",
    "                for f1 in range(f):\n",
    "                    grad_X[n1, : , h, m]=grad_X[n1, : , h, m]+ np.sum([W[f1, :, widx1[0], widx1[1]] * dout[n1, f1, yidx1[0], yidx1[1]] for widx1, yidx1 in zip(w_idx, y_idx)], 0)\n",
    "            \n",
    "    for f1 in range(f):\n",
    "        for c in range(C):\n",
    "             for i in range(HH):\n",
    "                for j in range(WW):\n",
    "                    grad_W[f1, c, i ,j] = np.sum(Xp[:,  c, i: i + H1 * stride : stride, j : j + W1* stride : stride] * dout[:, f1, :, :])\n",
    "    grad_b=np.sum(dout,axis=(0,2,3))\n",
    "\n",
    "    \n",
    "    #raise NotImplementedError()\n",
    "\n",
    "    # store the gradients\n",
    "    params['grad_W' + name] = grad_W\n",
    "    params['grad_b' + name] = grad_b\n",
    "    return grad_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batches(x: np.ndarray, y: np.ndarray, batch_size: int) -> list:\n",
    " \n",
    "    batches = []\n",
    "    # YOUR CODE HERE\n",
    "    i = range(x.shape[0])\n",
    "    while len(i)>0:\n",
    "        r=np.random.randint(0, len(i), batch_size)\n",
    "        a= [i[j] for j in r]\n",
    "        bx = [x[j] for j in a]\n",
    "        by = [y[j] for j in a]\n",
    "        batches.append((np.array(bx), np.array(by)))\n",
    "        i=list(set(i)-set(a))\n",
    "    #raise NotImplementedError()\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf4c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
